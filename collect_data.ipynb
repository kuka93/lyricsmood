{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.corpus import words\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The details (artist, title) of happy songs are collected by crawling last.fm website. The tags 'happy' and 'makes me happy' are used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816\n"
     ]
    }
   ],
   "source": [
    "happy_songs = []\n",
    "happy_urls = ['http://www.last.fm/tag/happy/tracks?page=', 'http://www.last.fm/tag/makes+me+happy/tracks?page=']\n",
    "for happy_url in happy_urls:\n",
    "    for i in range(1,21):\n",
    "        url = happy_url + str(i)\n",
    "        html = requests.get(url)\n",
    "        page = BeautifulSoup(html.text, 'lxml')\n",
    "        div = page.find('div', 'container page-content')\n",
    "        main_div = div.find('div', 'col-main')\n",
    "        table = main_div.find('tbody')\n",
    "\n",
    "        for row in table.findAll('tr'):\n",
    "            chartlist_name = row.find('td', 'chartlist-name')\n",
    "            main_span = chartlist_name.find('span', 'chartlist-ellipsis-wrap')\n",
    "            artist = chartlist_name.find('span', 'chartlist-artists').getText().strip()\n",
    "            title = main_span.findAll('a')[1].getText().strip()\n",
    "            if any(d['title'] == title and d['artist'] == artist for d in happy_songs) is False:\n",
    "                happy_songs.append({'artist':artist, 'title':title})\n",
    "print(len(happy_songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The details (artist, title) of sad songs are collected by crawling last.fm website. The tags 'sad', 'sad songs' and 'sadness' are used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2622\n"
     ]
    }
   ],
   "source": [
    "sad_songs = []\n",
    "sad_urls = ['http://www.last.fm/tag/sad/tracks?page=', 'http://www.last.fm/tag/sad+songs/tracks?page=', 'http://www.last.fm/tag/sadness/tracks?page=']\n",
    "for sad_url in sad_urls:\n",
    "    for i in range(1,21):\n",
    "        url = sad_url + str(i)\n",
    "        html = requests.get(url)\n",
    "        page = BeautifulSoup(html.text, 'lxml')\n",
    "        div = page.find('div', 'container page-content')\n",
    "        main_div = div.find('div', 'col-main')\n",
    "        table = main_div.find('tbody')\n",
    "\n",
    "        for row in table.findAll('tr'):\n",
    "            chartlist_name = row.find('td', 'chartlist-name')\n",
    "            main_span = chartlist_name.find('span', 'chartlist-ellipsis-wrap')\n",
    "            artist = chartlist_name.find('span', 'chartlist-artists').getText().strip()\n",
    "            title = main_span.findAll('a')[1].getText().strip()\n",
    "            if any(d['title'] == title and d['artist'] == artist for d in sad_songs) is False:\n",
    "                sad_songs.append({'artist':artist, 'title':title})\n",
    "print(len(sad_songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artists and titles need some preprocessing in order to build the proper URL to get the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_accent_letter(word):\n",
    "    word = word.replace('\\'','').replace('.','').replace('(','').replace(')','').replace('& ','').replace('?','').replace('.','')\n",
    "    word = word.replace('ö','o').replace('ó','o').replace('í','i').replace('é','e').replace('ü','u').replace('å','a').replace('ø','o')\n",
    "    return word\n",
    "\n",
    "def check_artist(word):\n",
    "    if word == 'the beatles':\n",
    "        return 'beatles'\n",
    "    elif word == 'the proclaimers':\n",
    "        return 'proclaimers'\n",
    "    elif word == 'the kooks':\n",
    "        return 'kooks'\n",
    "    elif word == 'katrina and the waves':\n",
    "        return 'katrina the waves'\n",
    "    elif word == 'the holloways':\n",
    "        return 'holloways'\n",
    "    elif word == 'sixpence none the richer':\n",
    "        return 'six pence none the richer'\n",
    "    elif word == 'the turtles':\n",
    "        return 'turtles'\n",
    "    elif word == 'the beach boys':\n",
    "        return 'beach boys'    \n",
    "    elif word == 'the monkees':\n",
    "        return 'monkees'\n",
    "    elif word == 'the jackson 5':\n",
    "        return 'jackson 5'\n",
    "    elif word == 'florence + the machine':\n",
    "        return 'florence and the machine'\n",
    "    elif word == 'the lovin\\' spoonful':\n",
    "        return 'lovin\\' spoonful'\n",
    "    elif word == 'karen o and the kids':\n",
    "        return 'karen-o-the-kids'\n",
    "    elif word == 'a-ha':\n",
    "        return 'aha'\n",
    "    elif word == 'the wombats':\n",
    "        return 'wombats'\n",
    "    elif word == 'the pipettes':\n",
    "        return 'pipettes'\n",
    "    elif word == 'the cardigans':\n",
    "        return 'cardigans'\n",
    "    elif word == 'the fratellis':\n",
    "        return 'fratellis'\n",
    "    elif word == 'the rolling stones':\n",
    "        return 'rolling stones'\n",
    "    elif word == 'simon & garfunkel':\n",
    "        return 'simon and garfunkel'\n",
    "    elif word == 'the b-52\\'s':\n",
    "        return 'b52s'\n",
    "    elif word == 'the la\\'s':\n",
    "        return 'las'\n",
    "    elif word == 'the dandy warhols':\n",
    "        return 'dandy warhols'\n",
    "    elif word == 'the flaming lips':\n",
    "        return 'flaming lips'\n",
    "    elif word == 'the temptations':\n",
    "        return 'temptations'\n",
    "    elif word == 'the strokes':\n",
    "        return 'strokes'\n",
    "    elif word == 'the cranberries':\n",
    "        return 'cranberries'\n",
    "    elif word == 'the apples in stereo':\n",
    "        return 'apples in stereo'\n",
    "    elif word == 'the feeling':\n",
    "        return 'feeling'\n",
    "    elif word == 'the smashing pumpkins':\n",
    "        return 'smashing pumpkins'\n",
    "    elif word == 'the darkness':\n",
    "        return 'darkness'\n",
    "    elif word == 'the kinks':\n",
    "        return 'kinks'\n",
    "    elif word == 'the magic numbers':\n",
    "        return 'magic numbers'\n",
    "    elif word == 'the all-american rejects':\n",
    "        return 'all-american rejects'\n",
    "    elif word == 'the rumble strips':\n",
    "        return 'rumble strips'\n",
    "    elif word == 'the decemberists':\n",
    "        return 'decemberists'\n",
    "    elif word == 'the boo radleys':\n",
    "        return 'boo radleys'\n",
    "    elif word == 'the presidents of the united states of america':\n",
    "        return 'presidents of the united states of america'\n",
    "    elif word == 'the little ones':\n",
    "        return 'little ones'\n",
    "    elif word == 'the j. geils band':\n",
    "        return 'j. geils band'\n",
    "    elif word == 'the champs':\n",
    "        return 'champs'\n",
    "    elif word == 'the pointer sisters':\n",
    "        return 'pointer sisters'\n",
    "    elif word == 'cansei de ser sexy':\n",
    "        return 'css'\n",
    "    elif word == 'the cat empire':\n",
    "        return 'cat empire'\n",
    "    elif word == 'daniel boone':\n",
    "        return 'boone daniel'\n",
    "    elif word == 'the velvet underground':\n",
    "        return 'velvet underground'\n",
    "    elif word == 'the pretenders':\n",
    "        return 'pretenders'\n",
    "    elif word == 'the housemartins':\n",
    "        return 'housemartins'\n",
    "    elif word == 'p!nk':\n",
    "        return 'pink'\n",
    "    elif word == 'the who':\n",
    "        return 'who'\n",
    "    elif word == 'the raveonettes':\n",
    "        return 'raveonettes'\n",
    "    elif word == 'the streets':\n",
    "        return 'streets'\n",
    "    elif word == 'the unicorns':\n",
    "        return 'unicorns'\n",
    "    elif word == 'the view':\n",
    "        return 'view'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def check_title(word):\n",
    "    if word == 'american boy (radio edit w/ kanye)':\n",
    "        return 'american boy'\n",
    "    elif word == 'the magic position':\n",
    "        return 'magic position'\n",
    "    elif word == 'mmmbop':\n",
    "        return 'mmm bop'\n",
    "    elif word == 'jungle drum':\n",
    "        return 'jungle drums'\n",
    "    elif word == 'all day and all of the night':\n",
    "        return 'all day all of the night'\n",
    "    elif word == 'light and day':\n",
    "        return 'light day'\n",
    "    elif word == 'yeah yeah yeah song':\n",
    "        return 'the yeah yeah yeah song'\n",
    "    elif word == 'the lovecats':\n",
    "        return 'lovecats'\n",
    "    elif word == 'the painter':\n",
    "        return 'the painter bonus track'\n",
    "    elif word == 'the lion sleeps tonight':\n",
    "        return 'lion sleeps tonight'\n",
    "    else:\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the title and artist contained in the songs dict. The url of the song is added to the song object in the dictionary\n",
    "\n",
    "## N.B. you should change happy/sad in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 0\n",
      "Song 100\n",
      "Song 200\n",
      "Song 300\n",
      "Song 400\n",
      "Song 500\n",
      "Song 600\n",
      "Song 700\n",
      "Song 800\n",
      "Song 900\n",
      "Song 1000\n",
      "Song 1100\n",
      "Song 1200\n",
      "Song 1300\n",
      "Song 1400\n",
      "Song 1500\n",
      "Song 1600\n",
      "Song 1700\n",
      "Song 1800\n",
      "Song 1900\n",
      "Song 2000\n",
      "Song 2100\n",
      "Song 2200\n",
      "Song 2300\n",
      "Song 2400\n",
      "Song 2500\n",
      "Song 2600\n"
     ]
    }
   ],
   "source": [
    "#change sad_songs/happy_songs\n",
    "for song in sad_songs:\n",
    "    if sad_songs.index(song) % 100 == 0:\n",
    "        print('Song %s' % (sad_songs.index(song)))\n",
    "    \n",
    "    artist = song['artist'].lower()\n",
    "    artist = check_artist(artist)\n",
    "    artist = replace_accent_letter(artist).replace(' ','-')\n",
    "    \n",
    "    title = song['title'].lower()\n",
    "    title = check_title(title)\n",
    "    title = replace_accent_letter(title).replace('-','').replace(',','').replace(' ','-')\n",
    "    \n",
    "    url = 'http://www.metrolyrics.com/'+title+'-lyrics-'+artist+'.html'\n",
    "    song['url'] = url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics collection\n",
    "\n",
    "### The lyrics are collected by crawling 'metrolyrics' website. Then lyrics are stored in a .txt file and we store the map (title,artist,url,path) in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 0\n",
      "Song 50\n",
      "Song 100\n",
      "Song 150\n",
      "Song 200\n",
      "Song 250\n",
      "Song 300\n",
      "Song 350\n",
      "Song 400\n",
      "Song 450\n",
      "Song 500\n",
      "Song 550\n",
      "Song 600\n",
      "Song 650\n",
      "Song 700\n",
      "Song 750\n",
      "Song 800\n",
      "Song 850\n",
      "Song 900\n",
      "Song 950\n",
      "Song 1000\n",
      "Song 1050\n",
      "Song 1100\n",
      "Song 1150\n",
      "Song 1200\n",
      "Song 1250\n",
      "Song 1300\n",
      "Song 1350\n",
      "Song 1400\n",
      "Song 1450\n",
      "Song 1500\n",
      "Song 1550\n",
      "Song 1600\n",
      "Song 1650\n",
      "Song 1700\n",
      "Song 1750\n",
      "Song 1800\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for song in happy_songs:\n",
    "    if happy_songs.index(song) % 50 == 0:\n",
    "        print('Song %s' % (happy_songs.index(song)))\n",
    "    \n",
    "    url = song['url']\n",
    "    data = list(song.values())\n",
    "    html = requests.get(url)\n",
    "    page = BeautifulSoup(html.text, 'lxml')\n",
    "    if html.status_code == 200:\n",
    "        main_div = page.find('div', 'lyrics-body')\n",
    "        if main_div == None:\n",
    "            continue\n",
    "        div_text = main_div.find('div', {'id' : 'lyrics-body-text'})\n",
    "        par = div_text.findAll('p')\n",
    "        txt = ''\n",
    "        for p in par:\n",
    "            txt += p.getText() + \" \"\n",
    "        \n",
    "        count += 1\n",
    "        filename = \"happy/song_\"+str(count)+\".txt\"\n",
    "\n",
    "        resultFile = open(\"output.csv\",'a')\n",
    "        data.append(filename)\n",
    "        wr = csv.writer(resultFile, dialect='excel', lineterminator='\\n')\n",
    "        wr.writerow(data)\n",
    "        resultFile.close()\n",
    "\n",
    "        with open(filename, \"w\") as text_file:\n",
    "            try:\n",
    "                text_file.write(\"%s\" % txt)\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 0\n",
      "Song 50\n",
      "Song 100\n",
      "Song 150\n",
      "Song 200\n",
      "Song 250\n",
      "Song 300\n",
      "Song 350\n",
      "Song 400\n",
      "Song 450\n",
      "Song 500\n",
      "Song 550\n",
      "Song 600\n",
      "Song 650\n",
      "Song 700\n",
      "Song 750\n",
      "Song 800\n",
      "Song 850\n",
      "Song 900\n",
      "Song 950\n",
      "Song 1000\n",
      "Song 1050\n",
      "Song 1100\n",
      "Song 1150\n",
      "Song 1200\n",
      "Song 1250\n",
      "Song 1300\n",
      "Song 1350\n",
      "Song 1400\n",
      "Song 1450\n",
      "Song 1500\n",
      "Song 1550\n",
      "Song 1600\n",
      "Song 1650\n",
      "Song 1700\n",
      "Song 1750\n",
      "Song 1800\n",
      "Song 1850\n",
      "Song 1900\n",
      "Song 1950\n",
      "Song 2000\n",
      "Song 2050\n",
      "Song 2100\n",
      "Song 2150\n",
      "Song 2200\n",
      "Song 2250\n",
      "Song 2300\n",
      "Song 2350\n",
      "Song 2400\n",
      "Song 2450\n",
      "Song 2500\n",
      "Song 2550\n",
      "Song 2600\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for song in sad_songs:\n",
    "    if sad_songs.index(song) % 50 == 0:\n",
    "        print('Song %s' % (sad_songs.index(song)))\n",
    "    \n",
    "    url = song['url']\n",
    "    data = list(song.values())\n",
    "    html = requests.get(url)\n",
    "    page = BeautifulSoup(html.text, 'lxml')\n",
    "    if html.status_code == 200:\n",
    "        main_div = page.find('div', 'lyrics-body')\n",
    "        if main_div == None:\n",
    "            continue\n",
    "        div_text = main_div.find('div', {'id' : 'lyrics-body-text'})\n",
    "        par = div_text.findAll('p')\n",
    "        txt = ''\n",
    "        for p in par:\n",
    "            txt += p.getText() + \" \"\n",
    "        \n",
    "        count += 1\n",
    "        filename = \"sad/song_\"+str(count)+\".txt\"\n",
    "\n",
    "        resultFile = open(\"output.csv\",'a')\n",
    "        data.append(filename)\n",
    "        wr = csv.writer(resultFile, dialect='excel', lineterminator='\\n')\n",
    "        wr.writerow(data)\n",
    "        resultFile.close()\n",
    "\n",
    "        with open(filename, \"w\") as text_file:\n",
    "            try:\n",
    "                text_file.write(\"%s\" % txt)\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a .txt file is empty (because it is not english or it has different encoding) and copy all non-empty files in a folder that then we use as a corpus directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "for file in os.listdir('happy'):\n",
    "    if os.path.getsize('happy/'+file) > 0:\n",
    "        shutil.copy2('happy/'+file, 'dataset/happy-'+file)\n",
    "for file in os.listdir('sad'):\n",
    "    if os.path.getsize('sad/'+file) > 0:\n",
    "        shutil.copy2('sad/'+file, 'dataset/sad-'+file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
