{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training/1\n",
    "\n",
    "### We use the stored files in the 'dataset' directory as our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "corpusdir = 'dataset/' # Directory of corpus.\n",
    "song_lyrics = PlaintextCorpusReader(corpusdir, '.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First of all, we train the model without preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['happy', 'sad']\n",
    "documents = []\n",
    "\n",
    "for fileid in song_lyrics.fileids():\n",
    "    for category in categories:\n",
    "        if category in fileid:\n",
    "            documents.append((list(song_lyrics.words(fileid)), category))\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', 'you', 'done', 'done', 'me', 'and', 'you', ...]\n"
     ]
    }
   ],
   "source": [
    "print(song_lyrics.words())\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in song_lyrics.words())\n",
    "most_common_words = all_words.most_common(1000)\n",
    "word_features = [x for (x, _) in most_common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our binary feature function\n",
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "import math\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "n = math.floor(len(featuresets) * 0.8)\n",
    "train_set, test_set = featuresets[:n], featuresets[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      |   h     |\n",
      "      |   a     |\n",
      "      |   p   s |\n",
      "      |   p   a |\n",
      "      |   y   d |\n",
      "------+---------+\n",
      "happy |<171>129 |\n",
      "  sad |  76<291>|\n",
      "------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "unlabeled_test_set = list()\n",
    "actual_label = list()\n",
    "\n",
    "for (d,c) in test_set:\n",
    "    unlabeled_test_set.append(d)\n",
    "    actual_label.append(c)\n",
    "    \n",
    "\n",
    "predicted_label = classifier.classify_many(unlabeled_test_set)\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = ConfusionMatrix(actual_label, predicted_label)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6926536731634183\n",
      "Precision is 0.6923076923076923\n",
      "Recall is 0.57\n",
      "F-measure is 0.6252285191956124\n"
     ]
    }
   ],
   "source": [
    "def print_precision_recall(cm):\n",
    "    precision = cm['happy', 'happy'] / (cm['happy', 'happy'] + cm['sad', 'happy'])\n",
    "    print(\"Precision is %s\" % precision)\n",
    "\n",
    "    recall = cm['happy', 'happy'] / (cm['happy', 'happy'] + cm['happy', 'sad'])\n",
    "    print(\"Recall is %s\" % recall)\n",
    "\n",
    "    f_measure = (2 * recall * precision) / (precision + recall)\n",
    "    print(\"F-measure is %s\" % f_measure)\n",
    "    \n",
    "print(\"Accuracy is %s\" % nltk.classify.accuracy(classifier, test_set))\n",
    "print_precision_recall(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we train the model preprocessing the documents. For preprocessing we can use various stemming methods. \n",
    "### The best found solution is: SnowballStemmer (ignoring the stemming of stopwords) and without stopwords removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 \n",
      "Document 200 \n",
      "Document 400 \n",
      "Document 600 \n",
      "Document 800 \n",
      "Document 1000 \n",
      "Document 1200 \n",
      "Document 1400 \n",
      "Document 1600 \n",
      "Document 1800 \n",
      "Document 2000 \n",
      "Document 2200 \n",
      "Document 2400 \n",
      "Document 2600 \n",
      "Document 2800 \n",
      "Document 3000 \n",
      "Document 3200 \n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "# This function does all the textual preprocessing steps\n",
    "def preprocessing(string_to_process):\n",
    "    normalized_string = string_to_process.lower().replace(\"_\", \"\")\n",
    "    # 1. Tokenize it! This also removes non alphanumeric characters since we're tokenizing words only.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(normalized_string)\n",
    "    \n",
    "    # 2. Remove stopwords\n",
    "    #filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # 3. Remove numbers\n",
    "    filtered_tokens = [word for word in tokens if not hasNumbers(word)]\n",
    "    \n",
    "    # 4. Stemm it!\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    #stemmer = LancasterStemmer()\n",
    "    #stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "    \n",
    "\n",
    "preprocessed_documents = list()\n",
    "\n",
    "for document in documents:\n",
    "    i = documents.index(document)\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Document %s \" % i)\n",
    "    document_text = ' '.join(document[0])\n",
    "    preprocessed_text = preprocessing(document_text)\n",
    "    preprocessed_documents.append((preprocessed_text, document[1]))\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get most frequent words from list of documents\n",
    "def get_most_frequent_words(documents, k=200):\n",
    "    all_words = list()\n",
    "    for document in documents:\n",
    "        for word in document[0]:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    all_words_frequency = nltk.FreqDist(all_words)\n",
    "    most_common_words = all_words_frequency.most_common(k)\n",
    "    return [x for (x, _) in most_common_words]\n",
    "\n",
    "word_features = get_most_frequent_words(preprocessed_documents, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in preprocessed_documents]\n",
    "n = math.floor(len(featuresets) * 0.8)\n",
    "train_set, test_set = featuresets[:n], featuresets[n:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      |   h     |\n",
      "      |   a     |\n",
      "      |   p   s |\n",
      "      |   p   a |\n",
      "      |   y   d |\n",
      "------+---------+\n",
      "happy |<168>132 |\n",
      "  sad |  77<290>|\n",
      "------+---------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy is 0.6866566716641679\n",
      "Precision is 0.6857142857142857\n",
      "Recall is 0.56\n",
      "F-measure is 0.6165137614678899\n",
      "None\n",
      "Most Informative Features\n",
      "           contains(woo) = True            happy : sad    =     13.0 : 1.0\n",
      "        contains(dancin) = True            happy : sad    =      9.4 : 1.0\n",
      "           contains(wow) = True            happy : sad    =      8.5 : 1.0\n",
      "            contains(eh) = True            happy : sad    =      8.5 : 1.0\n",
      "          contains(clap) = True            happy : sad    =      6.7 : 1.0\n",
      "          contains(boom) = True            happy : sad    =      6.2 : 1.0\n",
      "          contains(whoa) = True            happy : sad    =      6.1 : 1.0\n",
      "             contains(e) = True            happy : sad    =      5.9 : 1.0\n",
      "      contains(american) = True            happy : sad    =      5.8 : 1.0\n",
      "         contains(sugar) = True            happy : sad    =      5.6 : 1.0\n",
      "            contains(uh) = True            happy : sad    =      5.4 : 1.0\n",
      "            contains(ha) = True            happy : sad    =      5.2 : 1.0\n",
      "        contains(record) = True            happy : sad    =      5.1 : 1.0\n",
      "           contains(hip) = True            happy : sad    =      5.1 : 1.0\n",
      "            contains(ay) = True            happy : sad    =      4.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "unlabeled_test_set = list()\n",
    "actual_label = list()\n",
    "\n",
    "for (d,c) in test_set:\n",
    "    unlabeled_test_set.append(d)\n",
    "    actual_label.append(c)\n",
    "    \n",
    "\n",
    "predicted_label = classifier.classify_many(unlabeled_test_set)\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = ConfusionMatrix(actual_label, predicted_label)\n",
    "print(cm)\n",
    "print(\"Accuracy is %s\" % nltk.classify.accuracy(classifier, test_set))\n",
    "print(print_precision_recall(cm))\n",
    "print(classifier.show_most_informative_features(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we consider n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_k_best_bigrams_in_all_documents(documents, k=200):\n",
    "    all_words = list()\n",
    "    for document in documents:\n",
    "        for word in document[0]:\n",
    "            all_words.append(word)\n",
    "    all_bigrams = nltk.bigrams(all_words)\n",
    "    bigram_distributions = nltk.FreqDist(all_bigrams)\n",
    "    most_frequent_bigrams = bigram_distributions.most_common(k)\n",
    "    return [x for (x, _) in most_frequent_bigrams]\n",
    "\n",
    "bigram_features = find_k_best_bigrams_in_all_documents(preprocessed_documents, k=1000)\n",
    "\n",
    "def document_bigram_words_feature(document):\n",
    "    document_bigrams = list(nltk.bigrams(document))\n",
    "    result = dict((bigram,0) for bigram in bigram_features)\n",
    "    for bigram in result.keys():\n",
    "        result[bigram] = bigram in document_bigrams\n",
    "    return result\n",
    "\n",
    "def document_unigram_and_bigrams_words_feature(document):\n",
    "    unigram_feature_vector = document_features(document)\n",
    "    bigram_feature_vector = document_bigram_words_feature(document)\n",
    "    return {**unigram_feature_vector, **bigram_feature_vector}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      |   h     |\n",
      "      |   a     |\n",
      "      |   p   s |\n",
      "      |   p   a |\n",
      "      |   y   d |\n",
      "------+---------+\n",
      "happy |<173>127 |\n",
      "  sad |  75<292>|\n",
      "------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_unigram_and_bigrams_words_feature(d), c) for (d,c) in preprocessed_documents]\n",
    "n = math.floor(len(featuresets) * 0.8)\n",
    "train_set, test_set = featuresets[:n], featuresets[n:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "unlabeled_test_set = list()\n",
    "actual_label = list()\n",
    "\n",
    "for (d,c) in test_set:\n",
    "    unlabeled_test_set.append(d)\n",
    "    actual_label.append(c)\n",
    "    \n",
    "\n",
    "predicted_label = classifier.classify_many(unlabeled_test_set)\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = ConfusionMatrix(actual_label, predicted_label)\n",
    "print(cm)\n",
    "print(\"Accuracy is %s\" % nltk.classify.accuracy(classifier, test_set))\n",
    "print(print_precision_recall(cm))\n",
    "print(classifier.show_most_informative_features(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
